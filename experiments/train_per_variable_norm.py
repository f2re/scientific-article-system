"""
FINAL CORRECT version with PER-VARIABLE normalization.
Each variable (T, RH, Z, U, V) normalized separately.
Uses RELATIVE HUMIDITY (r) instead of specific humidity (q).
Enhanced with flexible temporal downloading parameters.
Supports both ERA5 and MERRA2 data sources with extension to 0.1 hPa.
"""
import warnings
warnings.filterwarnings('ignore')

import resource
import os
import json
import numpy as np
import xarray as xr
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from datetime import datetime
from pathlib import Path
import torch.backends.cudnn as cudnn
from torch.cuda.amp import autocast, GradScaler  # Mixed precision
# from model_architecture import AtmosphericProfileResNet
from model_architecture import  create_model, PhysicsInformedLoss


# ============================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–•
# ============================================================================

# –í–´–ë–û–† –ò–°–¢–û–ß–ù–ò–ö–ê –î–ê–ù–ù–´–•: 'ERA5' –∏–ª–∏ 'MERRA2'
DATA_SOURCE = 'MERRA2'  # –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ –Ω–∞ 'MERRA2' –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MERRA-2

# –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–∞–≥—Ä—É–∑–∫–∏
DOWNLOAD_YEARS = list(range(2015, 2021))  # 2015-2020

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–Ω—ã—Ö –º–µ—Å—è—Ü–µ–≤
SEASONAL_MONTHS = {
    'winter': ['12','01', '02'],      # –ó–∏–º–Ω–∏–µ –º–µ—Å—è—Ü—ã
    'spring': ['03','04', '05'],      # –í–µ—Å–µ–Ω–Ω–∏–µ –º–µ—Å—è—Ü—ã
    'summer': ['06','07', '08'],      # –õ–µ—Ç–Ω–∏–µ –º–µ—Å—è—Ü—ã
    'autumn': ['09','10', '11']       # –û—Å–µ–Ω–Ω–∏–µ –º–µ—Å—è—Ü—ã
}

# –í—ã–±–æ—Ä —Å–µ–∑–æ–Ω–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å)
SEASONS_TO_DOWNLOAD = ['winter', 'spring', 'summer', 'autumn']

# –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ä–µ–∑—ã –≤ —Å—É—Ç–∫–∞—Ö (—á–∞—Å—ã UTC)
TIME_SLICES = ['00:00', '12:00']

# –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –æ–±—ä–µ–º–∞)
SPATIAL_AREA = [90, -180, -90, 180]  # [North, West, South, East]

# –£—Ä–æ–≤–Ω–∏ –¥–∞–≤–ª–µ–Ω–∏—è (hPa) - —Ä–∞—Å—à–∏—Ä–µ–Ω—ã –¥–æ 0.1 –≥–ü–∞ –¥–ª—è –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
# ERA5 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É—Ä–æ–≤–Ω–∏ –æ—Ç 1000 –¥–æ 1 –≥–ü–∞ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–æ 0.1 –≥–ü–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ
PRESSURE_LEVELS_ERA5 = [
    '1', '2', '3', '5', '7', '10', '20', '30', '50', '70', '100',
    '125', '150', '175', '200', '225', '250', '300', '350', '400',
    '450', '500', '550', '600', '650', '700', '750', '775', '800',
    '825', '850', '875', '900', '925', '950', '975', '1000'
]
# –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: ERA5 –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —É—Ä–æ–≤–Ω–µ–π –≤—ã—à–µ 1 –≥–ü–∞.
# –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–æ 0.1 –≥–ü–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ MERRA2.

# MERRA2 –∏–∑–æ–±–∞—Ä–∏—á–µ—Å–∫–∏–µ —É—Ä–æ–≤–Ω–∏ (42 —É—Ä–æ–≤–Ω—è, –≤–∫–ª—é—á–∞—è 0.1 –≥–ü–∞)
PRESSURE_LEVELS_MERRA2 = [
    1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 725, 700,
    650, 600, 550, 500, 450, 400, 350, 300, 250, 200, 150, 100,
    70, 50, 40, 30, 20, 10, 7, 5, 4, 3, 2, 1, 0.7, 0.5, 0.4, 0.3, 0.1
]

BATCH_SIZE = 128
MAX_EPOCHS = 150

# Extended pressure levels to 0.1 hPa for both sources
# Input levels: troposphere (1000-100 hPa)
INPUT_LEVELS = [1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750,
                700, 650, 600, 550, 500, 450, 400, 350, 300, 250, 200, 150, 100]  # 24 levels

# Output levels: stratosphere and mesosphere (70-0.1 hPa)
OUTPUT_LEVELS = [70, 50, 40, 30, 20, 10, 7, 5, 4, 3, 2, 1, 0.7, 0.5, 0.4, 0.3, 0.1]  # 17 levels

# –ú–µ—Ç–µ–æ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ERA5
VARIABLES_ERA5 = [
    'geopotential',           # Z - –≥–µ–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Å–æ—Ç–∞
    'temperature',            # T - —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–æ–∑–¥—É—Ö–∞
    'u_component_of_wind',    # U - –∑–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤–µ—Ç—Ä–∞
    'v_component_of_wind',    # V - –º–µ—Ä–∏–¥–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤–µ—Ç—Ä–∞
    'relative_humidity',      # RH - –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å
]

# –ú–µ—Ç–µ–æ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ MERRA2 (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å ERA5)
VARIABLES_MERRA2 = {
    'H': 'geopotential',      # Z - –≥–µ–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –≤—ã—Å–æ—Ç–∞ (–±—É–¥–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞)
    'T': 'temperature',       # T - —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–æ–∑–¥—É—Ö–∞
    'U': 'u_component_of_wind',     # U - –∑–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤–µ—Ç—Ä–∞
    'V': 'v_component_of_wind',     # V - –º–µ—Ä–∏–¥–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤–µ—Ç—Ä–∞
    'RH': 'relative_humidity',      # RH - –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å
}

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è MERRA2 OPeNDAP
MERRA2_CONFIG = {
    'collection': 'M2I3NPASM',  # –ò–∑–æ–±–∞—Ä–∏—á–µ—Å–∫–∏–µ —É—Ä–æ–≤–Ω–∏, 3-—á–∞—Å–æ–≤—ã–µ –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    'server': 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap',
    'path': 'MERRA2/M2I3NPASM.5.12.4',
    'stream': 400  # –ù–æ–º–µ—Ä –ø–æ—Ç–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
}


def download_merra2_https(
    output_dir='./data/merra2',
    years=None,
    seasons=None,
    time_slices=None,
    days_per_month=None,
    day_step=10,
    skip_existing=True
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∞–Ω–∞–ª–∏–∑–∞ MERRA-2 —á–µ—Ä–µ–∑ HTTPS (–ø—Ä—è–º–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ).

    –í–ê–ñ–ù–û: NASA GES DISC –∏–∑–º–µ–Ω–∏–ª–∞ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç—É–ø–∞ - —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä—è–º–æ–µ
    —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ HTTPS –≤–º–µ—Å—Ç–æ OPeNDAP –¥–ª—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.

    Parameters
    ----------
    output_dir : str
        –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ NetCDF
    years : list of int, optional
        –°–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DOWNLOAD_YEARS
    seasons : list of str, optional
        –°–ø–∏—Å–æ–∫ —Å–µ–∑–æ–Ω–æ–≤ –∏–∑ SEASONAL_MONTHS. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é SEASONS_TO_DOWNLOAD
    time_slices : list of str, optional
        –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ä–µ–∑—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'HH:MM'. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é TIME_SLICES
    days_per_month : int, optional
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –∫–∞–∂–¥–æ–º –º–µ—Å—è—Ü–µ (1-31).
        None –æ–∑–Ω–∞—á–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –≤—Å–µ—Ö –¥–Ω–µ–π –º–µ—Å—è—Ü–∞ —Å —É—á–µ—Ç–æ–º day_step
    day_step : int
        –®–∞–≥ –≤—ã–±–æ—Ä–∫–∏ –¥–Ω–µ–π (1 = –≤—Å–µ –¥–Ω–∏, 10 = –∫–∞–∂–¥—ã–π 10-–π –¥–µ–Ω—å –∏ —Ç.–¥.)
    skip_existing : bool
        –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã

    Returns
    -------
    bool
        True –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ, False –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö
    """
    import subprocess
    import sys
    import requests

    print("\n" + "="*80)
    print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• MERRA-2 –ß–ï–†–ï–ó HTTPS (–ü–†–Ø–ú–û–ï –°–ö–ê–ß–ò–í–ê–ù–ò–ï)")
    print("="*80)
    print("\n‚ö† –í–ê–ñ–ù–û: –î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ MERRA-2 —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ ~/.netrc")
    print("   –§–∞–π–ª ~/.netrc –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:")
    print("   machine urs.earthdata.nasa.gov")
    print("       login YOUR_USERNAME")
    print("       password YOUR_PASSWORD")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–∞–≤–∞: chmod 600 ~/.netrc")
    print("="*80)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ ~/.netrc –∏ —á—Ç–µ–Ω–∏–µ credentials
    netrc_path = Path.home() / '.netrc'
    if not netrc_path.exists():
        print("\n‚úó –û–®–ò–ë–ö–ê: –§–∞–π–ª ~/.netrc –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("   –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ https://urs.earthdata.nasa.gov/users/new")
        print("   –∏ —Å–æ–∑–¥–∞–π—Ç–µ ~/.netrc —Å —É—á–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        return False

    # –ß–∏—Ç–∞–µ–º credentials –∏–∑ .netrc
    username = None
    password = None
    try:
        with open(netrc_path, 'r') as f:
            for line in f:
                if 'login' in line:
                    username = line.split()[1]
                if 'password' in line:
                    password = line.split()[1]

        if not username or not password:
            print("\n‚úó –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å credentials –∏–∑ ~/.netrc")
            return False

        print(f"\n‚úì Credentials –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {username}")

    except Exception as e:
        print(f"\n‚úó –û–®–ò–ë–ö–ê —á—Ç–µ–Ω–∏—è ~/.netrc: {e}")
        return False

    # –°–æ–∑–¥–∞–µ–º HTTP session —Å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    session = requests.Session()
    session.auth = (username, password)

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if years is None:
        years = DOWNLOAD_YEARS
    if seasons is None:
        seasons = SEASONS_TO_DOWNLOAD
    if time_slices is None:
        time_slices = TIME_SLICES

    if day_step < 1:
        print(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: day_step={day_step} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω day_step=1")
        day_step = 1

    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–µ—Å—è—Ü–µ–≤
    months_to_download = []
    for season in seasons:
        if season in SEASONAL_MONTHS:
            months_to_download.extend(SEASONAL_MONTHS[season])
        else:
            print(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–µ–∑–æ–Ω '{season}', –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è")

    months_to_download = sorted(list(set(months_to_download)))

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è TIME_SLICES –≤ —á–∞—Å—ã –¥–ª—è MERRA2 (MERRA2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —á–∞—Å—ã UTC: 0,3,6,9,12,15,18,21)
    merra2_hours = []
    for ts in time_slices:
        hour = int(ts.split(':')[0])
        # MERRA2 3-—á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: –≤—ã–±–∏—Ä–∞–µ–º –±–ª–∏–∂–∞–π—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —á–∞—Å
        available_hours = [0, 3, 6, 9, 12, 15, 18, 21]
        closest_hour = min(available_hours, key=lambda x: abs(x - hour))
        if closest_hour not in merra2_hours:
            merra2_hours.append(closest_hour)

    print(f"\n–ì–æ–¥—ã:      {years[0]}-{years[-1]} ({len(years)} –ª–µ—Ç)")
    print(f"–°–µ–∑–æ–Ω—ã:    {', '.join(seasons)}")
    print(f"–ú–µ—Å—è—Ü—ã:    {', '.join(months_to_download)}")
    print(f"–®–∞–≥ –ø–æ –¥–Ω—è–º: {day_step}")
    print(f"–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {len(VARIABLES_MERRA2)} ({', '.join(VARIABLES_MERRA2.keys())})")
    print(f"–£—Ä–æ–≤–Ω–∏:    {len(PRESSURE_LEVELS_MERRA2)} –∏–∑–æ–±–∞—Ä–∏—á–µ—Å–∫–∏—Ö (–¥–æ 0.1 –≥–ü–∞)")
    print(f"–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ä–µ–∑—ã: {merra2_hours} UTC")
    print(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    print("="*80)

    total_files = 0
    downloaded_files = 0
    skipped_files = 0
    error_files = 0

    # –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥–∞–º –∏ –º–µ—Å—è—Ü–∞–º
    for year in years:
        for month in months_to_download:
            month_int = int(month)

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π –≤ –º–µ—Å—è—Ü–µ
            if month_int in [1, 3, 5, 7, 8, 10, 12]:
                max_days = 31
            elif month_int in [4, 6, 9, 11]:
                max_days = 30
            elif month_int == 2:
                max_days = 29 if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0) else 28
            else:
                max_days = 31

            if days_per_month is not None:
                num_days = min(days_per_month, max_days)
            else:
                num_days = max_days

            days = list(range(1, num_days + 1, day_step))

            print(f"\n{year}-{month}: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–Ω–µ–π {days[:5]}{'...' if len(days) > 5 else ''}")

            for day in days:
                total_files += 1
                output_file = f'{output_dir}/merra2_pl_{year}{month}{day:02d}.nc'

                if skip_existing and os.path.exists(output_file):
                    skipped_files += 1
                    if total_files % 10 == 0:
                        print(f"[{total_files}] –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {year}-{month}-{day:02d}")
                    continue

                print(f"[{total_files}] –ó–∞–≥—Ä—É–∑–∫–∞: {year}-{month}-{day:02d}...")

                try:
                    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ URL –¥–ª—è HTTPS —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                    # –§–æ—Ä–º–∞—Ç: https://data.gesdisc.earthdata.nasa.gov/data/MERRA2/M2I3NPASM.5.12.4/YEAR/MONTH/FILENAME.nc4
                    date_str = f"{year}{month}{day:02d}"
                    filename = f"MERRA2_{MERRA2_CONFIG['stream']}.inst3_3d_asm_Np.{date_str}.nc4"
                    url = f"https://data.gesdisc.earthdata.nasa.gov/data/MERRA2/M2I3NPASM.5.12.4/{year}/{month}/{filename}"

                    # –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                    temp_file = f"{output_file}.tmp"

                    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ requests —Å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
                    print(f"  –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {filename}...")
                    response = session.get(url, stream=True, allow_redirects=True, timeout=300)

                    if response.status_code != 200:
                        error_files += 1
                        print(f"  ‚úó –û–®–ò–ë–ö–ê HTTP {response.status_code}: {response.reason}")
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                    total_size = int(response.headers.get('content-length', 0))
                    print(f"  –†–∞–∑–º–µ—Ä: {total_size / (1024*1024):.2f} MB")

                    # –°–∫–∞—á–∏–≤–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
                    downloaded = 0
                    with open(temp_file, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                                if total_size > 0 and downloaded % (10 * 1024 * 1024) == 0:  # –ö–∞–∂–¥—ã–µ 10 MB
                                    progress = (downloaded / total_size) * 100
                                    print(f"  –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%", end='\r')

                    print(f"  –°–∫–∞—á–∞–Ω–æ: 100%         ")

                    # –û—Ç–∫—Ä—ã—Ç—å —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å—É–±—Å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                    try:
                        ds = xr.open_dataset(
                            temp_file, 
                            decode_times=False,
                            engine='h5netcdf',     # –î–ª—è HDF5 —Ñ–∞–π–ª–æ–≤ MERRA-2
                            invalid_netcdf=True    # –†–∞–∑—Ä–µ—à–∏—Ç—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
                        )
                    except Exception as e_h5:
                        # Fallback –Ω–∞ netcdf4
                        try:
                            ds = xr.open_dataset(
                                temp_file, 
                                decode_times=False,
                                engine='netcdf4'
                            )
                        except Exception as e_nc4:
                            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª –Ω–∏ h5netcdf, –Ω–∏ netcdf4. –û—à–∏–±–∫–∏: h5={str(e_h5)[:100]}, nc4={str(e_nc4)[:100]}")

                    # –í—ã–±–æ—Ä –Ω—É–∂–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ —É—Ä–æ–≤–Ω–µ–π
                    var_list = list(VARIABLES_MERRA2.keys())

                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    vert_coord = 'lev' if 'lev' in ds.dims else 'level'

                    # –í—ã–±–æ—Ä –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ (–∏–Ω–¥–µ–∫—Å—ã –¥–ª—è merra2_hours)
                    time_indices = []
                    for hour in merra2_hours:
                        # MERRA2 3-—á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: 8 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–∑–æ–≤ –≤ –¥–µ–Ω—å
                        time_idx = hour // 3
                        if time_idx < len(ds.time):
                            time_indices.append(time_idx)

                    # –°—É–±—Å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                    ds_subset = ds[var_list].isel(time=time_indices)
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º method='nearest' –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å float32 —É—Ä–æ–≤–Ω—è–º–∏ MERRA2
                    ds_subset = ds_subset.sel({vert_coord: PRESSURE_LEVELS_MERRA2}, method='nearest')

                    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –ø–∞–º—è—Ç—å
                    ds_subset.load()

                    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è ERA5
                    rename_dict = {k: v for k, v in VARIABLES_MERRA2.items() if k in ds_subset}
                    ds_subset = ds_subset.rename(rename_dict)

                    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–µ–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –≤—ã—Å–æ—Ç—ã (H –≤ –º–µ—Ç—Ä–∞—Ö) –≤ –≥–µ–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª (—É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ g)
                    if 'geopotential' in ds_subset:
                        ds_subset['geopotential'] = ds_subset['geopotential'] * 9.80665
                        ds_subset['geopotential'].attrs['units'] = 'm**2 s**-2'
                        ds_subset['geopotential'].attrs['long_name'] = 'Geopotential'

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    ds_subset.to_netcdf(output_file)
                    downloaded_files += 1
                    print(f"  ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ: {output_file}")

                    # –ó–∞–∫—Ä—ã—Ç–∏–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    ds.close()
                    ds_subset.close()
                    if os.path.exists(temp_file):
                        os.remove(temp_file)

                except requests.exceptions.Timeout:
                    error_files += 1
                    print(f"  ‚úó –û–®–ò–ë–ö–ê: –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ")
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                    continue
                except requests.exceptions.RequestException as e:
                    error_files += 1
                    print(f"  ‚úó –û–®–ò–ë–ö–ê HTTP –∑–∞–ø—Ä–æ—Å–∞: {str(e)[:200]}")
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                    continue
                except Exception as e:
                    error_files += 1
                    print(f"  ‚úó –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)[:200]}")
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                    if os.path.exists(output_file):
                        os.remove(output_file)
                    continue

    print("\n" + "="*80)
    print("–ó–ê–ì–†–£–ó–ö–ê MERRA-2 –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*80)
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:          {total_files}")
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–æ–≤—ã—Ö:       {downloaded_files}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (—Å—É—â–µ—Å—Ç–≤.):  {skipped_files}")
    print(f"–û—à–∏–±–æ–∫:                {error_files}")
    print("="*80)

    return True


# ============================================================================
# –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
# ============================================================================

class AtmosphericDatasetPerVarNorm(Dataset):
    """
    Dataset with PER-VARIABLE normalization.
    Supports both ERA5 and MERRA2 data sources with extension to 0.1 hPa.
    """

    def __init__(self, data_files, input_levels, output_levels, stats=None, data_source='ERA5'):
        """
        Parameters
        ----------
        data_files : list
            List of NetCDF file paths
        input_levels : list
            Pressure levels for input (hPa)
        output_levels : list
            Pressure levels for output/target (hPa)
        stats : dict, optional
            Precomputed normalization statistics
        data_source : str
            Data source: 'ERA5' or 'MERRA2'
        """
        self.data_source = data_source
        self.input_levels = input_levels
        self.output_levels = output_levels
        self.n_input = len(input_levels)
        self.n_output = len(output_levels)
        self.var_names = ['t', 'r', 'z', 'u', 'v']
        self.profiles = []

        self._load_data(data_files)

        if stats is None:
            self.stats = self._compute_per_variable_stats()
        else:
            self.stats = stats

        self._normalize_all()

    def _load_data(self, data_files):
        """
        –£–°–¢–û–ô–ß–ò–í–ê–Ø –í–ï–†–°–ò–Ø —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π —Ä–µ—Å—É—Ä—Å–æ–≤.
        """
        import gc
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(data_files)} —Ñ–∞–π–ª–æ–≤ {self.data_source}...")
        print("–£–°–ö–û–†–ï–ù–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Å–±–æ–µ–≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã\n")
        
        var_mapping = {
            't': 'temperature',
            'r': 'relative_humidity',
            'z': 'geopotential',
            'u': 'u_component_of_wind',
            'v': 'v_component_of_wind'
        }
        
        all_levels = sorted(list(set(self.input_levels + self.output_levels)))
        n_input_levels = len(self.input_levels)
        rng = np.random.default_rng(42)
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        successful_files = 0
        failed_files = 0
        corrupted_files = []
        
        for file_idx, file_path in enumerate(data_files):
            ds = None  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            
            try:
                # ============================================================
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï #1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–¥ –æ—Ç–∫—Ä—ã—Ç–∏–µ–º
                # ============================================================
                if not os.path.exists(file_path):
                    print(f"  ‚ö† –§–∞–π–ª {file_idx+1}: –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    failed_files += 1
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ (–ø—É—Å—Ç—ã–µ/–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã)
                file_size = os.path.getsize(file_path)
                if file_size < 1000:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä NetCDF ~1KB
                    print(f"  ‚ö† –§–∞–π–ª {file_idx+1}: —Å–ª–∏—à–∫–æ–º –º–∞–ª ({file_size} bytes), —É–¥–∞–ª—è–µ–º")
                    os.remove(file_path)
                    corrupted_files.append(file_path)
                    failed_files += 1
                    continue
                
                # ============================================================
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï #2: –Ø–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                # ============================================================
                try:
                    ds = xr.open_dataset(
                        file_path,
                        decode_times=False,
                        engine='h5netcdf',
                        mask_and_scale=True,
                        phony_dims='sort',
                        # –ù–û–í–û–ï: –æ—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤
                        cache=False
                    )
                except (OSError, IOError) as e:
                    # Errno 107 –∏–ª–∏ –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –§–°
                    if 'errno = 107' in str(e) or 'Transport endpoint' in str(e):
                        print(f"  ‚ö† –§–∞–π–ª {file_idx+1}: –æ—à–∏–±–∫–∞ –§–° (errno 107), –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 2—Å...")
                        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ + –ø–∞—É–∑–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –§–°
                        gc.collect()
                        import time
                        time.sleep(2)
                        
                        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –û–î–ò–ù —Ä–∞–∑
                        try:
                            ds = xr.open_dataset(
                                file_path,
                                decode_times=False,
                                engine='h5netcdf',
                                cache=False
                            )
                        except Exception as retry_err:
                            print(f"  ‚úó –§–∞–π–ª {file_idx+1}: –ø–æ–≤—Ç–æ—Ä –Ω–µ—É–¥–∞—á–µ–Ω - {str(retry_err)[:100]}")
                            corrupted_files.append(file_path)
                            failed_files += 1
                            continue
                    else:
                        raise  # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–∞–ª—å—à–µ
                
                # ============================================================
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï #3: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–º–µ—Ä–µ–Ω–∏–π —Å fallback
                # ============================================================
                time_dim = next((c for c in ['valid_time', 'time', 't'] if c in ds.dims), None)
                level_dim = next((c for c in ['pressure_level', 'level', 'plev', 'lev'] if c in ds.dims), None)
                lat_dim = 'latitude' if 'latitude' in ds.dims else 'lat'
                lon_dim = 'longitude' if 'longitude' in ds.dims else 'lon'
                
                if not time_dim or not level_dim:
                    print(f"  ‚ö† –§–∞–π–ª {file_idx+1}: –Ω–µ—Ç –∏–∑–º–µ—Ä–µ–Ω–∏–π (time={time_dim}, level={level_dim})")
                    failed_files += 1
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –ø–µ—Ä–µ–¥ continue
                    if ds is not None:
                        ds.close()
                        ds = None
                    continue
                
                # ============================================================
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø #1: –û–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π sel –¥–ª—è –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
                # ============================================================
                ds_subset = ds.sel({level_dim: all_levels}, method='nearest')
                
                # –ù–û–í–û–ï: –Ø–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ –ø–∞–º—è—Ç—å + –∑–∞–∫—Ä—ã—Ç–∏–µ —Ñ–∞–π–ª–∞
                ds_subset = ds_subset.load()
                
                # –ö–†–ò–¢–ò–ß–ù–û: –ó–∞–∫—Ä—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π dataset –°–†–ê–ó–£ –ø–æ—Å–ª–µ load()
                ds.close()
                ds = None  # –û–±–Ω—É–ª—è–µ–º —Å—Å—ã–ª–∫—É
                
                n_times = min(2, len(ds_subset[time_dim]))
                
                for time_idx in range(n_times):
                    try:
                        # ============================================================
                        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø #2: –°—Ç–µ–∫ –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                        # ============================================================
                        var_arrays = []
                        for var_dataset in var_mapping.values():
                            if var_dataset not in ds_subset:
                                raise KeyError(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è {var_dataset} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                            var_arrays.append(ds_subset[var_dataset].isel({time_dim: time_idx}).values)
                        
                        all_vars_stack = np.stack(var_arrays, axis=0)
                        valid_mask = np.all(np.isfinite(all_vars_stack), axis=(0, 1))
                        
                        if not np.any(valid_mask):
                            continue
                        
                        # ============================================================
                        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø #3: –ü—Ä—è–º–∞—è —Å–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
                        # ============================================================
                        lat_indices, lon_indices = np.where(valid_mask)
                        n_valid = len(lat_indices)
                        n_samples = min(7000, n_valid)
                        selected = rng.choice(n_valid, n_samples, replace=False)
                        lat_sel = lat_indices[selected]
                        lon_sel = lon_indices[selected]
                        
                        if file_idx == 0 and time_idx == 0:
                            print(f"  –§–∞–π–ª 1, —Å—Ä–µ–∑ 0: {n_valid} –≤–∞–ª–∏–¥–Ω—ã—Ö ‚Üí –≤—ã–±—Ä–∞–Ω–æ {n_samples}")
                        
                        # ============================================================
                        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø #4: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è
                        # ============================================================
                        profiles_data = {}
                        for var_internal, var_dataset in var_mapping.items():
                            var_full = all_vars_stack[list(var_mapping.values()).index(var_dataset)]
                            profiles_var = var_full[:, lat_sel, lon_sel]
                            profiles_data[f'{var_internal}_input'] = profiles_var[:n_input_levels].T
                            profiles_data[f'{var_internal}_output'] = profiles_var[n_input_levels:].T
                        
                        for i in range(n_samples):
                            profile = {
                                key: arr[i].astype(np.float32)
                                for key, arr in profiles_data.items()
                            }
                            self.profiles.append(profile)
                    
                    except (KeyError, ValueError, IndexError) as e:
                        print(f"  ‚ö† –§–∞–π–ª {file_idx+1}, —Å—Ä–µ–∑ {time_idx}: –æ—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö - {str(e)[:80]}")
                        continue
                
                # –£—Å–ø–µ—à–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                successful_files += 1
                if (file_idx + 1) % 10 == 0 or file_idx < 5:
                    print(f"  –§–∞–π–ª {file_idx+1}/{len(data_files)}: {len(self.profiles)} –ø—Ä–æ—Ñ–∏–ª–µ–π –≤—Å–µ–≥–æ")
            
            except Exception as e:
                # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
                error_msg = str(e)[:200]
                print(f"  ‚úó –û–®–ò–ë–ö–ê —Ñ–∞–π–ª–∞ {file_idx+1}: {error_msg}")
                corrupted_files.append(file_path)
                failed_files += 1
            
            finally:
                # ============================================================
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï #4: –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                # ============================================================
                if ds is not None:
                    try:
                        ds.close()
                    except:
                        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏
                    finally:
                        ds = None
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –∫–∞–∂–¥—ã–µ 20 —Ñ–∞–π–ª–æ–≤
                if (file_idx + 1) % 20 == 0:
                    gc.collect()
        
        # ============================================================
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        # ============================================================
        print(f"\n{'='*80}")
        print(f"–ò–¢–û–ì–ò –ó–ê–ì–†–£–ó–ö–ò:")
        print(f"  ‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_files}/{len(data_files)}")
        print(f"  ‚úó –û—à–∏–±–æ–∫: {failed_files}")
        print(f"  üìä –í—Å–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª–µ–π: {len(self.profiles)}")
        
        if corrupted_files:
            print(f"\n‚ö† –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã ({len(corrupted_files)}):")
            for cf in corrupted_files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                print(f"    - {os.path.basename(cf)}")
            if len(corrupted_files) > 10:
                print(f"    ... –∏ –µ—â–µ {len(corrupted_files) - 10}")
        print(f"{'='*80}\n")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        if len(self.profiles) < 10000:
            raise RuntimeError(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(self.profiles)} –ø—Ä–æ—Ñ–∏–ª–µ–π "
                f"(–º–∏–Ω–∏–º—É–º 10,000). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–æ–≤."
            )
`



    def _compute_per_variable_stats(self):
        print("\n–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ü–û –ü–ï–†–ï–ú–ï–ù–ù–´–ú...")

        stats = {}

        for var in self.var_names:
            # input_data = np.concatenate([p[f'{var}_input'] for p in self.profiles])
            # output_data = np.concatenate([p[f'{var}_output'] for p in self.profiles])
            # –î–õ–Ø –¢–ï–ú–ü–ï–†–ê–¢–£–†–´: log-space –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            if var == 't':
                input_data = np.log(np.concatenate([p[f'{var}_input'] for p in self.profiles]))
                output_data = np.log(np.concatenate([p[f'{var}_output'] for p in self.profiles]))
            else:
                input_data = np.concatenate([p[f'{var}_input'] for p in self.profiles])
                output_data = np.concatenate([p[f'{var}_output'] for p in self.profiles])

            stats[var] = {
                'input_mean': float(input_data.mean()),
                'input_std': float(input_data.std()),
                'output_mean': float(output_data.mean()),
                'output_std': float(output_data.std())
            }

            print(f"  {var.upper()}: –≤—Ö_—Å—Ä–µ–¥–Ω–µ–µ={stats[var]['input_mean']:.2f}, –≤—ã—Ö_—Å—Ä–µ–¥–Ω–µ–µ={stats[var]['output_mean']:.2f}")

        return stats

    def _normalize_all(self):
        print("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º...")

        for profile in self.profiles:
            input_norm = []
            output_norm = []

            for var in self.var_names:
                var_input = profile[f'{var}_input']
                var_output = profile[f'{var}_output']

                var_input_norm = (var_input - self.stats[var]['input_mean']) / (self.stats[var]['input_std'] + 1e-6)
                var_output_norm = (var_output - self.stats[var]['output_mean']) / (self.stats[var]['output_std'] + 1e-6)

                input_norm.extend(var_input_norm)
                output_norm.extend(var_output_norm)

            profile['input_norm'] = np.array(input_norm, dtype=np.float32)
            profile['output_norm'] = np.array(output_norm, dtype=np.float32)

        print(f"–í—Å–µ {len(self.profiles)} –ø—Ä–æ—Ñ–∏–ª–µ–π –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã\n")

    def __len__(self):
        return len(self.profiles)

    def __getitem__(self, idx):
        profile = self.profiles[idx]
        return (
            torch.from_numpy(profile['input_norm']).float(),
            torch.from_numpy(profile['output_norm']).float()
        )


def train_model(model, train_loader, val_loader, device, max_epochs, output_dir):
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï #1: —É–º–µ–Ω—å—à–∞–µ–º learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    optimizer = optim.AdamW(model.parameters(), 
                        lr=3e-4,  # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ –≤—ã—à–µ –≤ 6 —Ä–∞–∑
                        betas=(0.9, 0.95),  # beta2 —Å–Ω–∏–∂–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        weight_decay=0.01)  # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    from torch.optim.lr_scheduler import OneCycleLR
    total_steps = max_epochs * len(train_loader)
    scheduler = OneCycleLR(
        optimizer,
        max_lr=3e-4,
        total_steps=total_steps,
        pct_start=0.1,  # 10% —ç–ø–æ—Ö –Ω–∞ warm-up
        anneal_strategy='cos',
        div_factor=25.0,  # –ù–∞—á–∞–ª—å–Ω—ã–π LR = max_lr/25 = 1.2e-5
        final_div_factor=1000.0  # –§–∏–Ω–∞–ª—å–Ω—ã–π LR = max_lr/1000 = 3e-7
    )
    
    criterion = PhysicsInformedLoss(
        n_output_levels=len(OUTPUT_LEVELS),
        thermal_wind_weight=0.5,  # –£–≤–µ–ª–∏—á–µ–Ω —Å 0.1 –¥–æ 0.5
        wind_component_weight=2.0,
        hydrostatic_weight=0.3  # –ù–û–í–´–ô –ø–∞—Ä–∞–º–µ—Ç—Ä
    ).to(device)
    
    use_amp = (device.type == 'cuda')
    scaler = GradScaler(enabled=use_amp)
    accumulation_steps = 1
    
    history = {
        'train_loss': [], 'val_loss': [], 'learning_rate': [],
        'train_mse_T': [], 'train_mse_U': [], 'train_mse_V': [],
        'val_mse_T': [], 'val_mse_U': [], 'val_mse_V': []
    }
    best_val_loss = float('inf')
    best_epoch = 0
    patience = 20  # Early stopping
    patience_counter = 0
    
    print(f"\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.2e} (—Å warm-up)")
    print(f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size: {BATCH_SIZE * accumulation_steps}")
    print(f"–§–∏–∑–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞: thermal_wind={criterion.tw_weight.item():.2f}, "
          f"hydrostatic={criterion.hs_weight.item():.2f}")
    print(f"Mixed Precision: {'Enabled' if use_amp else 'Disabled'}")
    print(f"Early Stopping: patience={patience}")
    print(f"{'='*80}\n")
    
    for epoch in range(1, max_epochs + 1):
        # ========== TRAINING ==========
        model.train()
        train_loss = 0.0
        train_metrics = {'mse_T': 0.0, 'mse_U': 0.0, 'mse_V': 0.0}
        optimizer.zero_grad(set_to_none=True)
        num_train_batches = 0
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            num_train_batches += 1
            inputs = inputs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
            if torch.isnan(inputs).any() or torch.isnan(targets).any():
                print(f"  WARNING: NaN –≤ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞ {batch_idx}, –ø—Ä–æ–ø—É—Å–∫")
                continue
            
            with autocast(enabled=use_amp):
                outputs = model(inputs)
                loss, loss_dict = criterion(outputs, targets)
                loss = loss / accumulation_steps
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf –≤ loss
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"  WARNING: NaN/Inf loss –≤ –±–∞—Ç—á–µ {batch_idx}, –ø—Ä–æ–ø—É—Å–∫")
                optimizer.zero_grad(set_to_none=True)
                continue
            
            scaler.scale(loss).backward()
            
            # Gradient accumulation
            if (batch_idx + 1) % accumulation_steps == 0:
                scaler.unscale_(optimizer)
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(f"  WARNING: NaN/Inf –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –±–∞—Ç—á–µ {batch_idx}, –ø—Ä–æ–ø—É—Å–∫")
                    optimizer.zero_grad(set_to_none=True)
                    scaler.update()
                    continue
                
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()  # –í–ê–ñ–ù–û: —à–∞–≥ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤
                optimizer.zero_grad(set_to_none=True)
            
            train_loss += loss.item() * accumulation_steps
            train_metrics['mse_T'] += loss_dict.get('mse_T', 0.0)
            train_metrics['mse_U'] += loss_dict.get('mse_U', 0.0)
            train_metrics['mse_V'] += loss_dict.get('mse_V', 0.0)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–∞ gradient accumulation
        if num_train_batches % accumulation_steps != 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad(set_to_none=True)
        
        if num_train_batches > 0:
            train_loss /= num_train_batches
            for key in train_metrics:
                train_metrics[key] /= num_train_batches
        
        # ========== VALIDATION ==========
        model.eval()
        val_loss = 0.0
        val_metrics = {'mse_T': 0.0, 'mse_U': 0.0, 'mse_V': 0.0}
        num_val_batches = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                num_val_batches += 1
                inputs = inputs.to(device, non_blocking=True)
                targets = targets.to(device, non_blocking=True)
                
                if torch.isnan(inputs).any() or torch.isnan(targets).any():
                    continue
                
                with autocast(enabled=use_amp):
                    outputs = model(inputs)
                    loss, loss_dict = criterion(outputs, targets)
                
                if not torch.isnan(loss):
                    val_loss += loss.item()
                    val_metrics['mse_T'] += loss_dict.get('mse_T', 0.0)
                    val_metrics['mse_U'] += loss_dict.get('mse_U', 0.0)
                    val_metrics['mse_V'] += loss_dict.get('mse_V', 0.0)
        
        if num_val_batches > 0:
            val_loss /= num_val_batches
            for key in val_metrics:
                val_metrics[key] /= num_val_batches
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        current_lr = optimizer.param_groups[0]['lr']
        history['train_loss'].append(float(train_loss))
        history['val_loss'].append(float(val_loss))
        history['learning_rate'].append(float(current_lr))
        history['train_mse_T'].append(train_metrics['mse_T'])
        history['train_mse_U'].append(train_metrics['mse_U'])
        history['train_mse_V'].append(train_metrics['mse_V'])
        history['val_mse_T'].append(val_metrics['mse_T'])
        history['val_mse_U'].append(val_metrics['mse_U'])
        history['val_mse_V'].append(val_metrics['mse_V'])
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if epoch % 5 == 0 or epoch == 1:
            print(f"–≠–ø–æ—Ö–∞ {epoch:3d}/{max_epochs} | "
                  f"Train: {train_loss:.6f} | Val: {val_loss:.6f} | "
                  f"LR: {current_lr:.2e} | "
                  f"ValMSE[T/U/V]: {val_metrics['mse_T']:.4f}/{val_metrics['mse_U']:.4f}/{val_metrics['mse_V']:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            patience_counter = 0
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'scaler_state_dict': scaler.state_dict(),
                'val_loss': val_loss,
                'train_loss': train_loss,
                'val_metrics': val_metrics
            }, os.path.join(output_dir, 'best_model.pth'))
            
            print(f"  ‚úì –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (—ç–ø–æ—Ö–∞ {epoch})")
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= patience:
            print(f"\nEarly stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch} (patience={patience})")
            break
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict()
    }, os.path.join(output_dir, 'final_model.pth'))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    with open(os.path.join(output_dir, 'training_history.json'), 'w') as f:
        json.dump({
            'metadata': {
                'best_epoch': best_epoch,
                'best_val_loss': float(best_val_loss),
                'final_epoch': epoch,
                'mixed_precision': use_amp,
                'accumulation_steps': accumulation_steps,
                'effective_batch_size': BATCH_SIZE * accumulation_steps
            },
            'history': history
        }, f, indent=2)
    
    print(f"\n{'='*80}")
    print(f"–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_epoch} | –õ—É—á—à–∏–π Val Loss: {best_val_loss:.6f}")
    print(f"{'='*80}\n")
    
    return history



def main():

    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        resource.setrlimit(resource.RLIMIT_NOFILE, (min(4096, hard), hard))
        print(f"‚úì –õ–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤—ã—Ö –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤: {soft} ‚Üí {min(4096, hard)}")
    except Exception as e:
        print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —É–≤–µ–ª–∏—á–∏—Ç—å –ª–∏–º–∏—Ç –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤: {e}")

    # Configuration based on DATA_SOURCE
    if DATA_SOURCE == 'ERA5':
        DATA_DIR = './data/era5'
        PRESSURE_LEVELS = PRESSURE_LEVELS_ERA5
        VARIABLES = VARIABLES_ERA5
    elif DATA_SOURCE == 'MERRA2':
        DATA_DIR = './data/merra2'
        PRESSURE_LEVELS = PRESSURE_LEVELS_MERRA2
        VARIABLES = list(VARIABLES_MERRA2.keys())
    else:
        raise ValueError(f"Unknown DATA_SOURCE: {DATA_SOURCE}. Must be 'ERA5' or 'MERRA2'")

    OUTPUT_DIR = f'./training_{DATA_SOURCE.lower()}_extended'

    n_variables = 5  # t, r, z, u, v
    input_dim = len(INPUT_LEVELS) * n_variables
    output_dim = len(OUTPUT_LEVELS) * n_variables

    torch.manual_seed(42)
    np.random.seed(42)

    # ==========================================================
    # GPU / Multi-GPU / DDP –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    # ==========================================================
    use_cuda = torch.cuda.is_available()
    world_size = int(os.environ.get("WORLD_SIZE", "1"))
    distributed = (world_size > 1) and use_cuda

    if use_cuda:
        cudnn.benchmark = True
        cudnn.deterministic = False

    if distributed:
        # torchrun –∑–∞–¥–∞—ë—Ç LOCAL_RANK, RANK, WORLD_SIZE
        local_rank = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(local_rank)
        device = torch.device("cuda", local_rank)

        torch.distributed.init_process_group(backend="nccl", init_method="env://")
        rank = torch.distributed.get_rank()

        if rank == 0:
            print(f"DDP: world_size={world_size}, local_rank={local_rank}")
    else:
        device = torch.device('cuda' if use_cuda else 'cpu')
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {device}")
        if use_cuda:
            print(f"GPU(0): {torch.cuda.get_device_name(0)}")
            print(f"–í—Å–µ–≥–æ GPU: {torch.cuda.device_count()}")

    print(f"\n–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {DATA_SOURCE}")
    print(f"–í—Ö–æ–¥–Ω—ã–µ —É—Ä–æ–≤–Ω–∏: {len(INPUT_LEVELS)} (–æ—Ç {INPUT_LEVELS[0]} –¥–æ {INPUT_LEVELS[-1]} –≥–ü–∞)")
    print(f"–í—ã—Ö–æ–¥–Ω—ã–µ —É—Ä–æ–≤–Ω–∏: {len(OUTPUT_LEVELS)} (–æ—Ç {OUTPUT_LEVELS[0]} –¥–æ {OUTPUT_LEVELS[-1]} –≥–ü–∞)")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞: {input_dim}")
    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—ã—Ö–æ–¥–∞: {output_dim}\n")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö {DATA_SOURCE}...\n")
    required_files = len(DOWNLOAD_YEARS) * len([m for s in SEASONS_TO_DOWNLOAD for m in SEASONAL_MONTHS[s]]) * 10
    existing_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.nc')] if os.path.exists(DATA_DIR) else []

    if len(existing_files) < required_files * 0.5:
        print(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö. –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ {DATA_SOURCE}...")
        print(f"–ë—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ ~{required_files} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∞ {DOWNLOAD_YEARS[0]}-{DOWNLOAD_YEARS[-1]}\n")
        
        download_merra2_https(
            output_dir=DATA_DIR,
            years=DOWNLOAD_YEARS,
            seasons=SEASONS_TO_DOWNLOAD,
            time_slices=TIME_SLICES,
            day_step=10,
            # days_per_month=1,
            skip_existing=True
        )
    else:
        print(f"‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(existing_files)} —Ñ–∞–π–ª–æ–≤ {DATA_SOURCE}\n")

    data_files = sorted([os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.nc')])
    if len(data_files) == 0:
        print("–û–®–ò–ë–ö–ê: –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return

    full_dataset = AtmosphericDatasetPerVarNorm(
        data_files, INPUT_LEVELS, OUTPUT_LEVELS,
        stats=None, data_source=DATA_SOURCE
    )

    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

    # hidden_dims
    if input_dim >= 145:
        hidden_dims = [768, 512, 384, 256, 384, 512, 768]
    else:
        hidden_dims = [512, 384, 256, 256, 384, 512]

    # –ö–æ–Ω—Ñ–∏–≥ –∏ —Å—Ç–∞—Ç—ã
    config = {
        'data_source': DATA_SOURCE,
        'input_levels': INPUT_LEVELS,
        'output_levels': OUTPUT_LEVELS,
        'input_dim': input_dim,
        'output_dim': output_dim,
        'n_variables': n_variables,
        'hidden_dims': hidden_dims
    }
    with open(os.path.join(OUTPUT_DIR, 'config.json'), 'w') as f:
        json.dump(config, f, indent=2)
    with open(os.path.join(OUTPUT_DIR, 'normalization_stats.json'), 'w') as f:
        json.dump(full_dataset.stats, f, indent=2)

    # –¢—Ä–µ–Ω/–≤–∞–ª–∏–¥ —Å–ø–ª–∏—Ç
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    # –ï—Å–ª–∏ DDP ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º DistributedSampler
    if distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=torch.distributed.get_rank(),
            shuffle=True,
            drop_last=True
        )
        val_sampler = torch.utils.data.distributed.DistributedSampler(
            val_dataset,
            num_replicas=world_size,
            rank=torch.distributed.get_rank(),
            shuffle=False,
            drop_last=False
        )
        shuffle_train = False
        shuffle_val = False
    else:
        train_sampler = None
        val_sampler = None
        shuffle_train = True
        shuffle_val = False

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=shuffle_train,
        sampler=train_sampler,
        drop_last=True,
        num_workers=4,
        pin_memory=use_cuda,
        persistent_workers=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=shuffle_val,
        sampler=val_sampler,
        num_workers=2,
        pin_memory=use_cuda,
        persistent_workers=False
    )

    # –ú–æ–¥–µ–ª—å
    model = create_model(
        input_dim=input_dim,
        output_dim=output_dim,
        n_input_levels=len(INPUT_LEVELS),
        n_output_levels=len(OUTPUT_LEVELS),
        device=device,
        output_pressure_levels=OUTPUT_LEVELS  # –ù–û–í–´–ô –ø–∞—Ä–∞–º–µ—Ç—Ä
    )

    model = model.to(device)

    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ GPU, –Ω–æ –Ω–µ DDP ‚Äî DataParallel
    if (not distributed) and use_cuda and torch.cuda.device_count() > 1:
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DataParallel –Ω–∞ {torch.cuda.device_count()} GPU")
        model = torch.nn.DataParallel(model)

    # –ï—Å–ª–∏ DDP, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ DistributedDataParallel
    if distributed:
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[device.index],
            output_device=device.index,
            find_unused_parameters=False
        )

    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}\n")

    # –í DDP `train_model` –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª–∏/–ª–æ–≥–∏ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª —Ç–æ–ª—å–∫–æ –Ω–∞ rank 0.
    if (not distributed) or (torch.distributed.get_rank() == 0):
        history = train_model(model, train_loader, val_loader, device, MAX_EPOCHS, OUTPUT_DIR)
        print(f"\n–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {OUTPUT_DIR}/best_model.pth, normalization_stats.json, config.json")
    else:
        # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ä–∞–Ω–∫–æ–≤ –ø—Ä–æ—Å—Ç–æ —Ç—Ä–µ–Ω–∏—Ä—É–µ–º –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        _ = train_model(model, train_loader, val_loader, device, MAX_EPOCHS, OUTPUT_DIR)

    if distributed:
        torch.distributed.destroy_process_group()


if __name__ == '__main__':
    main()
